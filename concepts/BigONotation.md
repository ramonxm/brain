# Big O Notation

### Big O **talks about how algorithms SCALE** depending on the **size** of the **input**, and **not** necessarily about the **performance** of the **algorithm**.

<aside>
ğŸ’¡

It's a way to **denote algorithm performance**, but it's **not** a **performance measure**

</aside>

<aside>
ğŸ’¡

Big O **doesn't measure the exact execution time** of an algorithm, but rather **how time grows** as input size increases. ğŸ“ˆ

</aside>

---

### Big O can be used to measure both *time complexity* and *space complexity* of an algorithm.

- **Time Complexity**: Relates to execution time (runtime), that is, how long it takes in *runtime.* **More common in interviews**
- **Space Complexity**: Relates to how much additional memory we need to allocate

---

> **Asymptotic analysis** is the way to analyze how an algorithm behaves in terms of time and memory as the input size grows.
> 

---

# Main BIG O notations:

## **O(1) â€“ Constant Complexity â†’**

> *"Turning on a light with a switch â€“ it always takes the same time, regardless of house size."*
> 
- **ğŸ§  Explained using Feynman Technique**
    
    Imagine you have a box of objects and need to find a specific item in it. Now, instead of searching through each item in the box, you already know exactly where it is. When opening the box, you go directly to the right item, without needing to check anything else. 
    The time you take to find the item is the same, no matter how many items are in the box. This is what happens with **O(1)** complexity. 
    Regardless of input size, the algorithm's execution time is constant.
    
    **O(1)** is constant complexity, meaning the algorithm's execution time doesn't depend on input size. Regardless of how many elements exist, the time or memory consumed is the same.
    
- **ğŸ§ Practical Example**
    - Accessing the first element of an array: `arr[0]`
    - Checking if a number is even: `num % 2 == 0`
    - Inserting or removing an item from the top of a stack.
- **â° About Time Complexity**
    
    **Constant time**, meaning regardless of input size, the algorithm has the same execution time.
    
    Execution time doesn't change, whether the input size is small or large. The algorithm always takes the same time to run.
    
    **â° Examples of O(1) â€“ Time**
    
    - Accessing a specific array index â†’ *arr[0]*
    - Checking if a number is even â†’ *num % 2 == 0*
    - Inserting/removing an element at the top of a stack
    - Example of O(1) algorithms â†’
        - Finding the first element of an array.
- **ğŸ‘©ğŸ¼â€ğŸš€ About Space Complexity**
    
    **Constant memory**, meaning allocated space doesn't grow with input.
    
    Memory space also doesn't vary with input size. The algorithm uses the same amount of memory regardless of the number of elements.
    
    ğŸ’¾ **Examples of O(1) â€“ Memory**
    
    - Creating a variable and storing a fixed value (int x = 10;)
    - Swapping values of two variables (a, b = b, a)
- **ğŸ” How to identify if it's O(1)?**
    
    **How to identify**:
    
    If the algorithm performs a single operation, regardless of input size, it's **O(1)**. In other words, the number of steps to complete the task doesn't change with increasing input.
    
    **Example**:
    
    - If you access an array element directly, like `arr[5]`, or swap two variable values, the time for these operations is constant.

---

## **O(log n) â€“ Logarithmic Complexity â†’**

> "*Finding a name in the phone book by dividing pages in half each time.*"
> 
- **ğŸ§  Explained using Feynman Technique**
    
    Imagine you have a book with **1,000 pages** and want to find a specific word. If you flip page by page, from start to finish, it could take a long time (O(n)).
    
    Now, imagine a smarter method: **you open the book in the middle** and see if the word is before or after that page. If it's before, you ignore the back half and look only at the front half. If it's after, ignore the front half and focus on the back.
    
    Repeat this process, cutting the problem in half at each step. In very few steps, you find the word! ğŸ¯
    
    ğŸ”¢ **This is O(log n)** â†’ Instead of processing **all** elements, you drastically reduce the amount of work at each step.
    
- **ğŸ§ Exemplo prÃ¡tico**
    - VocÃª tem um array ordenado e quer encontrar um nÃºmero.
    - Comece olhando o meio:
        - Se for o nÃºmero que procuramos, fim! ğŸ‰
        - Se o nÃºmero for menor, olhamos apenas a metade da esquerda.
        - Se for maior, olhamos apenas a metade da direita.
    - Em cada passo, cortamos o problema pela **metade** atÃ© encontrar o resultado.
- **â° Falando sobre complexidade Temporal**
    
    O tempo de execuÃ§Ã£o cresce muito mais devagar Ã  medida que o tamanho da entrada aumenta, em comparaÃ§Ã£o com algoritmos **O(n)** ou **O(nÂ²)**.
    
    â° **Exemplos de O(log n) - Temporal**:
    
    - **Busca binÃ¡ria**: A cada iteraÃ§Ã£o, o nÃºmero de elementos possÃ­veis pela metade.
    - **Encontrar o elemento no meio de uma lista ordenada** e eliminar metade das opÃ§Ãµes em cada iteraÃ§Ã£o.
- **ğŸ‘©ğŸ¼â€ğŸš€ Falando sobre complexide Espacial**
    
    A memÃ³ria utilizada por um algoritmo **O(log n)** tambÃ©m cresce de forma logarÃ­tmica em relaÃ§Ã£o ao tamanho da entrada. Isso significa que, mesmo que o nÃºmero de elementos cresÃ§a, o espaÃ§o utilizado por cada operaÃ§Ã£o nÃ£o aumenta drasticamente.
    
    **ğŸ‘©ğŸ¼â€ğŸš€ Exemplos de O(log n) - Espacial**:
    
    - **RecursÃ£o em busca binÃ¡ria**, onde a pilha de chamadas cresce logaritmicamente Ã  medida que o tamanho da entrada diminui.
    - Algoritmos de **divisÃ£o e conquista** podem precisar de espaÃ§o extra para armazenar subproblemas, mas o espaÃ§o usado nÃ£o cresce linearmente.
- **ğŸ” Como descobrir se Ã© O(log n)?**
    
    **Como identificar**:
    
    Se vocÃª perceber que, a cada passo, o tamanho do problema diminui significativamente â€” por exemplo, sempre pela metade â€” o algoritmo provavelmente tem complexidade **O(log n)**. Isso acontece porque o nÃºmero de iteraÃ§Ãµes necessÃ¡rias cresce muito mais devagar do que o tamanho da entrada.
    
    **Exemplo**:
    
    - Se vocÃª estÃ¡ dividindo o problema pela metade a cada iteraÃ§Ã£o, como em uma busca binÃ¡ria, o tempo de execuÃ§Ã£o serÃ¡ **O(log n)**.
    - Quando o algoritmo realiza uma busca ou divisÃ£o de dados que elimina metade das opÃ§Ãµes a cada passo, como na **busca binÃ¡ria**, vocÃª pode identificar que Ã© **O(log n)**.

---

## **O(n) â€“ Complexidade Linearâ†’**

> "*Ler um livro do comeÃ§o ao fim sem pular pÃ¡ginas.*â€
> 
- **ğŸ§  Explicado usando Feynman Technique**
    
    Imagine que vocÃª tem uma linha de pessoas esperando para fazer uma tarefa. Se a pessoa na frente terminar, ela chama a prÃ³xima, e assim por diante, atÃ© que todos tenham feito a tarefa. O tempo necessÃ¡rio para todos fazerem a tarefa depende diretamente de quantas pessoas estÃ£o na fila. Se houver mais pessoas na fila, o tempo total aumenta na mesma proporÃ§Ã£o. 
    Esse Ã© o conceito de **O(n)**: o tempo de execuÃ§Ã£o aumenta linearmente com o tamanho da entrada. 
    O nÃºmero de operaÃ§Ãµes que o algoritmo realiza aumenta conforme o nÃºmero de itens que ele precisa processar.
    
- **ğŸ§ Exemplo prÃ¡tico**
    
    Passo a passo:
    
    1. VocÃª tem um array de 10 elementos e quer somar todos os nÃºmeros nele.
    2. VocÃª comeÃ§a do primeiro elemento e vai atÃ© o Ãºltimo, somando um por um.
    3. O nÃºmero de passos (operaÃ§Ãµes) que o algoritmo faz Ã© igual ao nÃºmero de elementos no array.
    4. Portanto, se o array tiver 100 elementos, o algoritmo farÃ¡ 100 operaÃ§Ãµes; se tiver 1.000, ele farÃ¡ 1.000 operaÃ§Ãµes.
    
    **Outro exemplo:**
    
    - **Buscar um elemento em um array nÃ£o ordenado**: O algoritmo precisa olhar cada elemento atÃ© encontrar o que estÃ¡ procurando.
    - **Imprimir todos os elementos de um array**: O tempo de execuÃ§Ã£o aumenta com o nÃºmero de elementos que vocÃª precisa imprimir.
- **â° Falando sobre complexidade Temporal**
    
    **O(n)** Ã© linear. O tempo de execuÃ§Ã£o cresce na mesma proporÃ§Ã£o que o tamanho da entrada. 
    Se vocÃª tem 1.000 itens para processar, o algoritmo farÃ¡ 1.000 operaÃ§Ãµes, e se vocÃª tiver 10.000 itens, farÃ¡ 10.000 operaÃ§Ãµes.
    
    â° **Exemplos de O(n) - Temporal**:
    
    - **Soma de todos os elementos de um array**: O algoritmo percorre cada elemento uma vez para somÃ¡-los.
    - **Buscar um item em um array nÃ£o ordenado**: O algoritmo verifica cada elemento atÃ© encontrar o item.
    - **Imprimir cada valor de uma lista**: A operaÃ§Ã£o de imprimir cada item exige uma operaÃ§Ã£o por item na lista.
- **ğŸ‘©ğŸ¼â€ğŸš€ Falando sobre complexide Espacial**
    - **Armazenar cÃ³pias dos elementos de uma lista**: Se o algoritmo precisa criar uma nova lista para armazenar os elementos, o espaÃ§o necessÃ¡rio cresce com o nÃºmero de elementos.
    - **Contar o nÃºmero de vezes que um valor aparece em um array**: Para cada item que o algoritmo verifica, ele precisa de um pouco mais de memÃ³ria para armazenar a contagem.
- **ğŸ” Como descobrir se Ã© O(n)?**
    
    **Como identificar**:
    
    Se o algoritmo faz uma operaÃ§Ã£o para cada item na entrada (ou em uma parte significativa da entrada), o tempo de execuÃ§Ã£o Ã© **O(n)**. O nÃºmero de operaÃ§Ãµes cresce proporcionalmente ao tamanho da entrada.
    
    **Exemplo**:
    
    - Se vocÃª precisa percorrer uma lista e realizar uma operaÃ§Ã£o em cada item (como somar ou verificar um valor), isso serÃ¡ **O(n)** porque o nÃºmero de operaÃ§Ãµes aumenta conforme o tamanho da lista.

---

## **O(n log n) â€“ Complexidade Quasilinear â†’**

> â€œ*Organizar um baralho dividindo as cartas em pilhas menores e juntando depois.*â€
> 
- **ğŸ§  Explicado usando Feynman Technique**
    
    Imagine que vocÃª tem um grande nÃºmero de dados e quer organizÃ¡-los de alguma forma, como em um array. Agora, em vez de olhar para todos os dados de uma vez, vocÃª divide em partes menores e faz algo com essas partes (como ordenÃ¡-las) para depois juntar tudo de novo. 
    Esse processo de dividir, ordenar e juntar faz o tempo de execuÃ§Ã£o crescer de maneira um pouco mais complexa do que um simples **O(n)**, mas muito mais rÃ¡pido do que o **O(nÂ²)**. 
    
    O "log n" vem do processo de dividir repetidamente os dados em partes menores, e o "n" vem de ter que processar todos os dados ao longo do caminho.
    
- **ğŸ§ Exemplo prÃ¡tico**
    - Exemplos de algoritmos que usam:
        - SORTING (quicksort, mergesort)
        - DIVEDE AND CONQUER
    - Digamos que vocÃª tenha um array de **n** elementos.
    - VocÃª divide esse array em dois subarrays menores atÃ© que cada subarray tenha um Ãºnico elemento.
    - Depois, vocÃª comeÃ§a a juntar esses subarrays ordenando-os, de forma que no final vocÃª tenha o array inteiro ordenado.
    - O tempo para dividir os arrays Ã© **O(log n)**, e o tempo para percorrer todos os **n** elementos enquanto vocÃª faz a ordenaÃ§Ã£o Ã© **O(n)**.
    - Juntando os dois, temos **O(n log n)**.
- **â° Falando sobre complexidade Temporal**
    
    **O(n log n)** significa que o tempo de execuÃ§Ã£o aumenta conforme o nÃºmero de elementos cresce, mas de uma forma mais controlada do que **O(nÂ²)**. 
    
    A ordem de grandeza do tempo de execuÃ§Ã£o Ã© mais eficiente, o que faz esse tipo de algoritmo ser muito Ãºtil para problemas grandes, como ordenaÃ§Ã£o de dados.
    
    â° **Exemplos de O(n log n) - Temporal**
    
    - **Merge Sort**: Um dos algoritmos mais conhecidos para ordenar dados com **O(n log n)**.
    - **Quick Sort**: Outro algoritmo de ordenaÃ§Ã£o que tem um desempenho de **O(n log n)**, mas em alguns casos pode ter desempenho pior se nÃ£o for bem implementado.
- **ğŸ‘©ğŸ¼â€ğŸš€ Falando sobre complexide Espacial**
    
    **O(n log n)** pode envolver o uso de memÃ³ria extra para armazenar os subarrays temporÃ¡rios durante o processo de divisÃ£o e junÃ§Ã£o. 
    Dependendo do algoritmo, pode ser necessÃ¡rio mais espaÃ§o para armazenar esses dados temporÃ¡rios.
    
    ğŸ’¾ **Exemplos de O(n log n) - Espacial**
    
    - **Merge Sort**: Durante o processo de ordenaÃ§Ã£o, o **Merge Sort** precisa de espaÃ§o extra proporcional a **O(n)** para armazenar subarrays temporÃ¡rios.
    - **Quick Sort**: O espaÃ§o extra usado pelo **Quick Sort** pode ser **O(log n)** no melhor caso, mas pode ser maior dependendo de como a recursÃ£o Ã© realizada.
- **ğŸ” Como descobrir se Ã© O(n log n)?**
    - Para identificar se um algoritmo tem **O(n log n)** de complexidade, Ã© importante procurar por processos que envolvem dividir um conjunto de dados em partes menores e realizar uma operaÃ§Ã£o sobre cada uma dessas partes, como ordenar. 
    Esses algoritmos tendem a ter uma combinaÃ§Ã£o de um loop que percorre todos os **n** dados e um processo de divisÃ£o recursiva ou logarÃ­tmica.
    
    **Como identificar:**
    
    1. Se vocÃª vÃª uma **divisÃ£o repetitiva** dos dados, como no caso de **Merge Sort** ou **Quick Sort**.
    2. Se cada operaÃ§Ã£o de divisÃ£o tem um custo de **O(log n)**, mas vocÃª ainda precisa passar por todos os dados **n**.
    3. Quando o tempo de execuÃ§Ã£o Ã© uma combinaÃ§Ã£o de linear e logarÃ­tmica, como **n log n**.

---

## **O(nÂ²) â€“ Complexidade QuadrÃ¡tica â†’**

> "*Comparar cada aluno da turma com todos os outros para descobrir quem Ã© mais alto.*â€
> 
- **ğŸ§  Explicado usando Feynman Technique**
    
    **Viu loop dentro de um loop ou Bubble sort? Pode cravar que Ã© O(nÂ²)!**
    Imagine que vocÃª tem uma lista de pessoas e quer comparar cada uma delas com todas as outras para encontrar pares com algo em comum. No comeÃ§o, vocÃª compara a primeira pessoa com todas as outras, depois a segunda com todas as outras (exceto a primeira, porque jÃ¡ foi comparada), e assim por diante. 
    Isso cria uma cascata de comparaÃ§Ãµes, onde cada novo elemento faz o nÃºmero de interaÃ§Ãµes crescer de forma quadrÃ¡tica. Se tivermos 10 elementos, fazemos **100 comparaÃ§Ãµes**; se tivermos 1.000, fazemos **1.000.000**! 
    Isso explica por que algoritmos **O(nÂ²)** ficam lentos com entradas grandes.
    
- **ğŸ§ Exemplo prÃ¡tico**
    - VocÃª tem um array de **n** elementos.
    - VocÃª precisa comparar cada elemento com todos os outros.
    - O primeiro elemento serÃ¡ comparado com **n - 1** elementos.
    - O segundo elemento serÃ¡ comparado com **n - 2** elementos.
    - Isso se repete atÃ© o Ãºltimo elemento, criando **n Ã— n** operaÃ§Ãµes.
- **â° Falando sobre complexidade Temporal**
    
    **O(nÂ²)** significa que Ã  medida que **n** cresce, o tempo de execuÃ§Ã£o cresce proporcionalmente ao quadrado desse nÃºmero. 
    Isso significa que um pequeno aumento no tamanho do input pode resultar em um aumento **exponencial** no tempo de execuÃ§Ã£o.
    
    â° **Exemplos de O(nÂ²) - Temporal**
    
    - **Bubble Sort**: Cada elemento precisa ser comparado com todos os outros.
    - **Selection Sort**: Busca pelo menor elemento e o troca de posiÃ§Ã£o vÃ¡rias vezes.
    - **Algoritmo ingÃªnuo para encontrar pares duplicados**: Um loop dentro de outro loop para verificar se existem elementos iguais.
- **ğŸ‘©ğŸ¼â€ğŸš€ Falando sobre complexide Espacial**
    
    **O(nÂ²)** pode, em alguns casos, exigir uma grande quantidade de memÃ³ria extra se for necessÃ¡rio armazenar pares, matrizes ou listas auxiliares durante as operaÃ§Ãµes.
    
    ğŸ’¾ **Exemplos de O(nÂ²) - Espacial**
    
    - **MultiplicaÃ§Ã£o de Matrizes IngÃªnua**: Cada cÃ©lula da matriz resultante requer mÃºltiplas iteraÃ§Ãµes pelos dados.
    - **CriaÃ§Ã£o de tabelas auxiliares** para armazenar relaÃ§Ãµes entre elementos (exemplo: uma tabela de adjacÃªncia em grafos).
- **ğŸ” Como descobrir se Ã© O(nÂ²)?**
    - Para identificar se um algoritmo tem **O(nÂ²)** de complexidade, procure por **loops aninhados** onde cada iteraÃ§Ã£o percorre toda a entrada para cada elemento.
    
    **Como identificar:**
    
    1. Se hÃ¡ **dois loops aninhados** que percorrem o mesmo conjunto de dados.
    2. Se a quantidade de operaÃ§Ãµes cresce **muito rÃ¡pido** conforme o input aumenta.
    3. Se um problema envolve comparar **todos os elementos com todos os outros** (como em alguns algoritmos de ordenaÃ§Ã£o).

---

# Outras notaÃ§Ãµes do BIG O (menos comuns):

## **O(2â¿) â€“ Complexidade Exponencial â†’**

> "*Testar todas as formas possÃ­veis de resolver um cubo mÃ¡gico.*â€
> 
- **ğŸ§  Explicado usando Feynman Technique**
    
    Imagine que vocÃª tem um cofre e quer descobrir a senha, mas nÃ£o sabe quantos nÃºmeros precisa testar. 
    
    Cada vez que vocÃª adiciona um dÃ­gito a mais, o nÃºmero de possibilidades dobra. Se a senha tiver 1 dÃ­gito, hÃ¡ 2 possibilidades (0 ou 1). 
    
    Se tiver 2 dÃ­gitos, hÃ¡ 4 possibilidades (00, 01, 10, 11). Com 3 dÃ­gitos, jÃ¡ sÃ£o 8 possibilidades. 
    
    Assim, conforme o nÃºmero de elementos aumenta, o nÃºmero de operaÃ§Ãµes cresce exponencialmente, dobrando a cada nova unidade adicionada.
    
- **ğŸ§ Exemplo prÃ¡tico**
    - VocÃª precisa gerar **todas as combinaÃ§Ãµes possÃ­veis** de um conjunto.
    - Se hÃ¡ **n elementos**, cada um pode estar **presente ou ausente** na combinaÃ§Ã£o.
    - Isso cria **2â¿ possibilidades** de subconjuntos.
    - Para **n = 3**, as combinaÃ§Ãµes possÃ­veis seriam:
        - {}
        - {A}
        - {B}
        - {C}
        - {A, B}
        - {A, C}
        - {B, C}
        - {A, B, C}
    - Para **n = 4**, jÃ¡ terÃ­amos **16 possibilidades**.
    - Para **n = 10**, jÃ¡ seriam **1024 possibilidades**. ğŸ˜¯
- **â° Falando sobre complexidade Temporal**
    
    **O(2â¿)** significa que, conforme o input cresce, o tempo de execuÃ§Ã£o **dobra** a cada novo elemento. Isso torna esses algoritmos extremamente ineficientes para inputs grandes.
    
    â° **Exemplos de O(2â¿) - Temporal**
    
    - **Problema do subconjunto**: Gerar todas as combinaÃ§Ãµes possÃ­veis de um conjunto.
    - **Problema da Mochila (Brute Force)**: Testar todas as combinaÃ§Ãµes possÃ­veis de itens para encontrar o melhor conjunto.
    - **Fibonacci Recursivo (sem otimizaÃ§Ã£o)**: A versÃ£o ingÃªnua da sequÃªncia de Fibonacci, onde cada chamada recursiva cria duas novas chamadas.
- **ğŸ‘©ğŸ¼â€ğŸš€ Falando sobre complexide Espacial**
    
    **O(2â¿)** pode exigir **muito espaÃ§o na memÃ³ria**, especialmente quando todas as possibilidades precisam ser armazenadas antes da execuÃ§Ã£o.
    
    ğŸ’¾ **Exemplos de O(2â¿) - Espacial**
    
    - **Armazenamento de todas as combinaÃ§Ãµes de um conjunto**.
    - **Backtracking em problemas de busca exaustiva**, onde todas as opÃ§Ãµes precisam ser mantidas na pilha de chamadas da recursÃ£o.
- **ğŸ” Como descobrir se Ã© O(2â¿)?**
    - Se o nÃºmero de operaÃ§Ãµes **dobra** a cada novo elemento adicionado, vocÃª provavelmente estÃ¡ lidando com **O(2â¿)**.
    
    **Como identificar:**
    
    1. Se o algoritmo envolve **gerar todas as combinaÃ§Ãµes possÃ­veis** de um conjunto.
    2. Se a soluÃ§Ã£o usa **recursÃ£o exponencial**, onde cada chamada gera **duas ou mais chamadas recursivas**.
    3. Se a quantidade de operaÃ§Ãµes cresce **extremamente rÃ¡pido**, tornando o algoritmo inviÃ¡vel para inputs grandes.

---

## **O(âˆšn) â€“ Complexidade de Raiz Quadrada â†’**

> â€œ*Subir uma escada pulando degraus para chegar mais rÃ¡pido ao topo.*â€
> 
- **ğŸ§  Explicado usando Feynman Technique**
    
    Imagine que vocÃª precisa encontrar um nÃºmero especÃ­fico dentro de **n** elementos, mas, ao invÃ©s de verificar todos um por um (**O(n)**), vocÃª pode **pular** de um grupo para outro, reduzindo drasticamente a quantidade de verificaÃ§Ãµes. 
    Por exemplo, se vocÃª tivesse **100 elementos**, ao invÃ©s de checar todos, vocÃª poderia **dividir em blocos** e testar apenas **âˆš100 = 10 elementos**. 
    Esse comportamento aparece em problemas onde Ã© possÃ­vel reduzir a busca para a raiz quadrada do input.
    
- **ğŸ§ Exemplo prÃ¡tico**
    
    
- **â° Falando sobre complexidade Temporal**
    - VocÃª precisa encontrar um nÃºmero **primo** menor que **n**.
    - Ao invÃ©s de testar divisibilidade atÃ© **n**, vocÃª sÃ³ precisa verificar atÃ© **âˆšn**.
    - Exemplo: Para verificar se **37 Ã© primo**, vocÃª nÃ£o precisa testar divisibilidade de **1 a 37**. Basta testar atÃ© **âˆš37 â‰ˆ 6** (2, 3 e 5).
    - Isso reduz a quantidade de verificaÃ§Ãµes **significativamente** para nÃºmeros grandes.
- **ğŸ‘©ğŸ¼â€ğŸš€ Falando sobre complexide Espacial**
    
    **O(âˆšn)** significa que, conforme **n cresce**, o tempo de execuÃ§Ã£o **cresce mais devagar** do que O(n), mas **mais rÃ¡pido** que O(log n).
    
    â° **Exemplos de O(âˆšn) - Temporal**
    
    - **VerificaÃ§Ã£o de nÃºmeros primos**: Testar divisibilidade apenas atÃ© **âˆšn**.
    - **Jump Search**: Algoritmo de busca em um array ordenado que pula **âˆšn** posiÃ§Ãµes a cada iteraÃ§Ã£o.
    - **Problemas de fatoraÃ§Ã£o**: Ao fatorar um nÃºmero, o maior fator primo possÃ­vel serÃ¡ no mÃ¡ximo **âˆšn**.
- **ğŸ” Como descobrir se Ã© O(âˆšn)?**
    - Se o nÃºmero de operaÃ§Ãµes cresce **na mesma proporÃ§Ã£o que a raiz quadrada de n**, o algoritmo provavelmente tem complexidade **O(âˆšn)**.
    
    **Como identificar:**
    
    1. Se o algoritmo **divide o problema em blocos** de tamanho **âˆšn** e processa apenas esses blocos.
    2. Se hÃ¡ uma **reduÃ§Ã£o significativa na quantidade de iteraÃ§Ãµes** comparado com O(n), mas sem atingir a eficiÃªncia de O(log n).
    3. Se a soluÃ§Ã£o envolve **operaÃ§Ãµes matemÃ¡ticas relacionadas Ã  raiz quadrada**, como fatoraÃ§Ã£o ou verificaÃ§Ã£o de primos.

---

## **O(N!) â€“ Complexidade Fatorial â†’**

> "*Planejar uma viagem visitando todas as cidades possÃ­veis em todas as ordens possÃ­veis.*â€
> 
- **ğŸ§  Explicado usando Feynman Technique**
    
    Imagine que vocÃª precisa organizar **n** pessoas em uma fila, mas cada vez que adiciona uma nova pessoa, o nÃºmero de formas de organizÃ¡-las **explode**. 
    Se houver **3 pessoas**, temos **3! = 6 formas** de organizÃ¡-las. 
    Se forem **4 pessoas**, temos **4! = 24 formas**. 
    Agora imagine com **10 pessoas**: seriam **3.628.800 combinaÃ§Ãµes**! 
    O crescimento Ã© **absurdamente rÃ¡pido** e fica impraticÃ¡vel para valores grandes de **n**.
    
- **ğŸ§ Exemplo prÃ¡tico**
    - VocÃª precisa gerar **todas as permutaÃ§Ãµes possÃ­veis** de um conjunto de **n** elementos.
    - Para **3 elementos** (*A, B, C*), as permutaÃ§Ãµes sÃ£o:
        1. ABC
        2. ACB
        3. BAC
        4. BCA
        5. CAB
        6. CBA
    - Se adicionarmos **D**, o nÃºmero sobe para **24 permutaÃ§Ãµes (4!)**.
- **â° Falando sobre complexidade Temporal**
    
    O tempo de execuÃ§Ã£o cresce **de forma absurda** conforme **n aumenta**. Um algoritmo **O(n!)** se torna **impraticÃ¡vel muito rÃ¡pido**.
    
    â° **Exemplos de O(n!) - Temporal**
    
    - **Gerar todas as permutaÃ§Ãµes possÃ­veis** de um conjunto.
    - **Resolver o problema do Caixeiro Viajante (TSP) por forÃ§a bruta**, testando todas as rotas possÃ­veis.
    - **Quebra de senha por brute-force** se for necessÃ¡rio testar todas as combinaÃ§Ãµes possÃ­veis de caracteres.
- **ğŸ‘©ğŸ¼â€ğŸš€ Falando sobre complexide Espacial**
    
    **O(n!)** tambÃ©m pode exigir um consumo gigantesco de memÃ³ria se armazenarmos todas as permutaÃ§Ãµes possÃ­veis.
    
    ğŸ’¾ **Exemplos de O(n!) - Espacial**
    
    - **Armazenar todas as permutaÃ§Ãµes** de uma sequÃªncia de elementos.
    - **Backtracking com estados salvos** em problemas combinatÃ³rios pode levar a um espaÃ§o de busca **explosivo**.
- **ğŸ” Como descobrir se Ã© O(N!)?**
    - Se o nÃºmero de operaÃ§Ãµes cresce **multiplicando sucessivamente por n, n-1, n-2, ..., 1**, entÃ£o o algoritmo provavelmente tem complexidade **O(n!)**.
    
    **Como identificar:**
    
    1. Se o algoritmo **gera todas as permutaÃ§Ãµes possÃ­veis** de um conjunto.
    2. Se hÃ¡ **muitas escolhas recursivas** e a Ã¡rvore de recursÃ£o cresce **muito rapidamente**.
    3. Se a soluÃ§Ã£o envolve **testar todas as combinaÃ§Ãµes possÃ­veis** de um problema combinatÃ³rio.